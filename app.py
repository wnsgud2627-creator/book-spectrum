import streamlit as st
import pandas as pd
import requests
from google import genai
import json
import io
import re
import time
import os

# ==========================================
# 1. ì„¤ì • ë° API ì´ˆê¸°í™”
# ==========================================
ALADIN_TTB_KEY = st.secrets["ALADIN_TTB_KEY"]
GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
MODEL_ID = 'models/gemini-2.5-flash'

DEFAULT_KEYWORDS = (
    "ë§ˆìŒ, ìš©ê¸°, í–‰ë³µ, ì‚¬ë‘, ê°ì •, ìì‹ ê°, ì •ì§, ì•½ì†, ë‚˜ëˆ”, ë°°ë ¤, ì„±ì¥, í˜¸ê¸°ì‹¬, "
    "ê°€ì¡±, ì¹œêµ¬, ì´ì›ƒ, ìœ ì¹˜ì›, ì„ ìƒë‹˜, í˜‘ë ¥, ì¸ì‚¬, ì˜ˆì ˆ, í•¨ê»˜, ë„ì›€, "
    "ìŒì‹, ê±´ê°•, ì²­ê²°, í¸ì‹, ì ìê¸°, í™”ì¥ì‹¤, ì•ˆì „, ì˜· ì…ê¸°, ê·œì¹™, ìƒí™œìŠµê´€, "
    "ë™ë¬¼, ê³¤ì¶©, ë°”ë‹¤, ìˆ², ì‹ë¬¼, ê³„ì ˆ, ë‚ ì”¨, ìš°ì£¼, ì§€êµ¬, í™˜ê²½, ê³µë£¡, ê³¼í•™, "
    "ìƒìƒ, ëª¨í—˜, ìƒ‰ê¹”, ì†Œë¦¬, ê·¸ë¦¬ê¸°, ë§Œë“¤ê¸°, ìŒì•…, ë§ˆë²•, ì˜›ì´ì•¼ê¸°, ì „í†µ"
)

@st.cache_resource
def init_gemini_client():
    return genai.Client(api_key=GOOGLE_API_KEY)

client = init_gemini_client()

# ==========================================
# 2. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜
# ==========================================

def get_book_info_aladin(title, author=""):
    url = "http://www.aladin.co.kr/ttb/api/ItemSearch.aspx"
    clean_title = re.sub(r'\(.*?\)|\[.*?\]', '', str(title))
    query = f"{clean_title} {str(author).strip()}"
    params = {
        'ttbkey': ALADIN_TTB_KEY, 'Query': query, 'QueryType': 'Keyword',
        'MaxResults': 1, 'Output': 'js', 'SearchTarget': 'Book',
        'Version': '20131101', 'OptResult': 'Story,fulldescription'
    }
    try:
        response = requests.get(url, params=params, timeout=5)
        data = json.loads(response.text.strip().rstrip(';'))
        if 'item' in data and data['item']:
            item = data['item'][0]
            desc = f"{item.get('description', '')} {item.get('fullDescription', '')} {item.get('story', '')}"
            return {"isbn13": item.get('isbn13', '-'), "desc": re.sub(r'<[^>]*>', ' ', desc)}
    except: pass
    return None

def refine_with_gemini(book_data, title, keyword_pool):
    if not client or not book_data: return None
    prompt = f"""
    ë‹¹ì‹ ì€ 4~7ì„¸ ë„ì„œ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ë¼ë²¨ëŸ¬ì…ë‹ˆë‹¤. '{title}' ì •ë³´ ê¸°ë°˜ìœ¼ë¡œ ì‘ì—…í•˜ì„¸ìš”.
    [ê·œì¹™] 
    1. ì¤„ê±°ë¦¬: ë‹¤ì •í•œ ì„ ìƒë‹˜ ë§íˆ¬, 3ë¬¸ì¥ ë‚´ì™¸, ë§ˆì§€ë§‰ì€ ì§ˆë¬¸.
    2. í‚¤ì›Œë“œ: [í‘œì¤€ ëª©ë¡]ì—ì„œ 3ê°œ í•„ì„ íƒ, ë³¸ë¬¸ ì†Œì¬ 2ê°œ ì„ íƒ (ì´ 5ê°œ).
    [í‘œì¤€ ëª©ë¡]: {keyword_pool}
    ì •ë³´ ì›ë¬¸: {book_data['desc'][:1000]}
    ì‘ë‹µ í˜•ì‹(JSON): {{"summary": "ë‚´ìš©", "keywords": ["k1", "k2", "k3", "k4", "k5"]}}
    """
    try:
        response = client.models.generate_content(model=MODEL_ID, contents=prompt)
        json_text = re.search(r'\{.*\}', response.text, re.DOTALL)
        return json.loads(json_text.group()) if json_text else None
    except Exception: return None

# ==========================================
# 3. ìŠ¤íŠ¸ë¦¼ë¦¿ UI
# ==========================================
st.set_page_config(page_title="Book Spectrum v3.0", layout="wide")
st.title("ğŸŒˆ ë¶ ìŠ¤í™íŠ¸ëŸ¼ v3.0")

DAILY_MAX_LIMIT = 2500  # í•˜ë£¨ ìµœëŒ€ ë¶„ì„ ê¶Œìˆ˜ (ì•ˆì „ì¥ì¹˜)

with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    user_keyword_list = st.text_area("í‘œì¤€ í‚¤ì›Œë“œ ì‚¬ì „ ê´€ë¦¬", value=DEFAULT_KEYWORDS, height=200)
    st.divider()
    uploaded_file = st.file_uploader("ì›ë³¸ ì—‘ì…€ ì—…ë¡œë“œ", type=["xlsx"])
    start_btn = st.button("ğŸš€ ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True)

if uploaded_file:
    # (ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë¶€ë¶„ ìƒëµ - ê¸°ì¡´ê³¼ ë™ì¼)
    
    if start_btn:
        progress_bar = st.progress(0)
        analyzed_count = 0  # ì´ë²ˆ ì‹¤í–‰ì—ì„œ ë¶„ì„í•œ ê¶Œìˆ˜ ì¹´ìš´íŠ¸
        
        for i, row in st.session_state.display_df.iterrows():
            # ì´ë¯¸ ë¶„ì„ëœ ê±´ ê±´ë„ˆëœ€
            if row['ì•„ì´ìš© ì¤„ê±°ë¦¬'] not in ["ëŒ€ê¸° ì¤‘...", "ê²€ìƒ‰ ì‹¤íŒ¨", "ë¶„ì„ ì‹¤íŒ¨"]:
                continue
            
            # ğŸ›‘ í• ë‹¹ëŸ‰ ì²´í¬
            if analyzed_count >= DAILY_MAX_LIMIT:
                st.error(f"âš ï¸ ì¼ì¼ ë¶„ì„ í•œë„({DAILY_MAX_LIMIT}ê¶Œ)ì— ë„ë‹¬í•˜ì—¬ ì¤‘ë‹¨í•©ë‹ˆë‹¤. ì„¤ì •ê°’ì€ ì½”ë“œì—ì„œ ìˆ˜ì • ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                break
            
            title = str(row.get('ë„ì„œëª…', '')).strip()
            author = str(row.get('ì €ì', row.get('ê¸€ì“´ì´', ''))).strip()
            
            # ë¶„ì„ ì‹¤í–‰
            info = get_book_info_aladin(title, author)
            if info:
                st.session_state.display_df.at[i, 'ISBN13'] = info.get('isbn13')
                refined = refine_with_gemini(info, title, user_keyword_list)
                if refined:
                    st.session_state.display_df.at[i, 'ì•„ì´ìš© ì¤„ê±°ë¦¬'] = refined.get('summary')
                    st.session_state.display_df.at[i, 'ì¶”ì²œ í‚¤ì›Œë“œ'] = ", ".join(refined.get('keywords', []))
                    analyzed_count += 1  # ë¶„ì„ ì„±ê³µ ì‹œ ì¹´ìš´íŠ¸ ì¦ê°€
                else:
                    st.session_state.display_df.at[i, 'ì•„ì´ìš© ì¤„ê±°ë¦¬'] = "ë¶„ì„ ì‹¤íŒ¨"
            else:
                st.session_state.display_df.at[i, 'ì•„ì´ìš© ì¤„ê±°ë¦¬'] = "ê²€ìƒ‰ ì‹¤íŒ¨"
            
            # í…Œì´ë¸” ì—…ë°ì´íŠ¸ ë° ì§„í–‰ë°”
            table_placeholder.dataframe(st.session_state.display_df, use_container_width=True)
            progress_bar.progress((i + 1) / len(st.session_state.display_df))
            time.sleep(0.5) # ìœ ë£Œ í‹°ì–´ì´ë¯€ë¡œ ì†ë„ë¥¼ ì•½ê°„ ë†’ì—¬ë„ ë©ë‹ˆë‹¤ (ê¸°ì¡´ 1ì´ˆ -> 0.5ì´ˆ)

        st.success(f"âœ… ë¶„ì„ ì™„ë£Œ! (ì´ë²ˆ ì„¸ì…˜ì—ì„œ ì´ {analyzed_count}ê¶Œ ë¶„ì„ë¨)")

    with tab2:
        st.subheader("ğŸ“Œ í‚¤ì›Œë“œ ë¶„í¬ ë° ë„ì„œ í•„í„°ë§")
        kw_series = st.session_state.display_df['ì¶”ì²œ í‚¤ì›Œë“œ'].dropna()
        all_keywords = []
        for kw_str in kw_series:
            if kw_str != "ëŒ€ê¸° ì¤‘...":
                all_keywords.extend([k.strip() for k in kw_str.split(",")])
        
        if all_keywords:
            kw_counts = pd.Series(all_keywords).value_counts().reset_index()
            kw_counts.columns = ['í‚¤ì›Œë“œ', 'ìˆ˜ëŸ‰']
            
            col1, col2 = st.columns([1, 2])
            with col1:
                st.write("âœ… í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (í•„í„°ë§í•  í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ì„¸ìš”)")
                # í‚¤ì›Œë“œ ì„ íƒìš© ì…€ë ‰íŠ¸ë°•ìŠ¤ ì¶”ê°€
                selected_keyword = st.selectbox("ì¡°íšŒí•  í‚¤ì›Œë“œ ì„ íƒ", ["ì „ì²´ ë³´ê¸°"] + list(kw_counts['í‚¤ì›Œë“œ']))
                st.dataframe(kw_counts, use_container_width=True, height=300)
            
            with col2:
                st.write("ğŸ“ˆ í‚¤ì›Œë“œ ë¶„í¬ ì°¨íŠ¸")
                st.bar_chart(kw_counts.set_index('í‚¤ì›Œë“œ').head(15))
            
            st.divider()
            
            # í•„í„°ë§ ê²°ê³¼ ì¶œë ¥
            st.subheader(f"ğŸ“– '{selected_keyword}' í‚¤ì›Œë“œ í¬í•¨ ë„ì„œ ëª©ë¡")
            if selected_keyword == "ì „ì²´ ë³´ê¸°":
                st.dataframe(st.session_state.display_df, use_container_width=True)
            else:
                filtered_df = st.session_state.display_df[
                    st.session_state.display_df['ì¶”ì²œ í‚¤ì›Œë“œ'].str.contains(selected_keyword, na=False)
                ]
                st.dataframe(filtered_df, use_container_width=True)
        else:
            st.info("ë¶„ì„ì´ ì™„ë£Œë˜ë©´ í‚¤ì›Œë“œ í†µê³„ì™€ í•„í„°ë§ ê¸°ëŠ¥ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")

    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        st.session_state.display_df.to_excel(writer, index=False)
    st.download_button("ğŸ“¥ ìµœì¢… ê²°ê³¼ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", data=output.getvalue(), file_name="Book_Spectrum_Final.xlsx", use_container_width=True)
